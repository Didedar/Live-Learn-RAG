#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ Ollama –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Live-Learn RAG —Å–∏—Å—Ç–µ–º–æ–π.
"""

import asyncio
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, Any

import httpx
from loguru import logger

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2:latest"  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ llama3.2:3b –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã
ENV_FILE = ".env"


def check_ollama_installed() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ Ollama –≤ —Å–∏—Å—Ç–µ–º–µ."""
    try:
        result = subprocess.run(["ollama", "--version"],
                                capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            logger.info(f"Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {result.stdout.strip()}")
            return True
        else:
            logger.error("Ollama –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–∏—Å—Ç–µ–º–µ")
            return False
    except (subprocess.TimeoutExpired, FileNotFoundError):
        logger.error("Ollama –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–∏—Å—Ç–µ–º–µ")
        return False


def install_ollama_instructions():
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ Ollama."""
    logger.info("=" * 60)
    logger.info("üìã –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –£–°–¢–ê–ù–û–í–ö–ï OLLAMA:")
    logger.info("=" * 60)
    logger.info("")
    logger.info("üçé macOS:")
    logger.info("   brew install ollama")
    logger.info("   # –∏–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ —Å https://ollama.ai")
    logger.info("")
    logger.info("üêß Linux:")
    logger.info("   curl -fsSL https://ollama.ai/install.sh | sh")
    logger.info("")
    logger.info("ü™ü Windows:")
    logger.info("   –°–∫–∞—á–∞–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫ —Å https://ollama.ai")
    logger.info("")
    logger.info("–ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞.")
    logger.info("=" * 60)


async def check_ollama_running() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä Ollama."""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{OLLAMA_URL}/api/tags")
            response.raise_for_status()
            logger.info("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")
            return True
    except Exception as e:
        logger.warning(f"Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç: {e}")
        return False


def start_ollama_server():
    """–ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä Ollama."""
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Ollama —Å–µ—Ä–≤–µ—Ä...")

        process = subprocess.Popen(
            ["ollama", "serve"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )

        logger.info(f"Ollama —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω —Å PID: {process.pid}")
        logger.info("–ñ–¥–µ–º 5 —Å–µ–∫—É–Ω–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞...")

        import time
        time.sleep(5)

        return True

    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä: {e}")
        return False


async def list_available_models() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ Ollama."""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{OLLAMA_URL}/api/tags")
            response.raise_for_status()

            data = response.json()
            models = [model["name"] for model in data.get("models", [])]

            logger.info(f"üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ Ollama: {models}")
            return {"models": models, "total": len(models)}

    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
        return {"models": [], "total": 0}


async def pull_model(model_name: str) -> bool:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ Ollama."""
    try:
        logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å {model_name}...")
        logger.info("‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏...")

        async with httpx.AsyncClient(timeout=600.0) as client:  # 10 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
            async with client.stream(
                "POST",
                f"{OLLAMA_URL}/api/pull",
                json={"name": model_name}
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            import json
                            data = json.loads(line)
                            if "status" in data:
                                status = data["status"]
                                if "total" in data and "completed" in data:
                                    total = data["total"]
                                    completed = data["completed"]
                                    percent = (completed / total * 100) if total > 0 else 0
                                    logger.info(f"   üìä {status}: {percent:.1f}%")
                                else:
                                    logger.info(f"   üìù {status}")
                        except:
                            continue

        logger.success(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞!")
        return True

    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}: {e}")
        return False


async def test_model(model_name: str) -> bool:
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏."""
    try:
        logger.info(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å {model_name}...")

        test_prompt = "–°–∫–∞–∂–∏—Ç–µ '–ü—Ä–∏–≤–µ—Ç' –µ—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ."

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_URL}/api/generate",
                json={
                    "model": model_name,
                    "prompt": test_prompt,
                    "stream": False
                }
            )
            response.raise_for_status()

            result = response.json()
            answer = result.get("response", "").strip()

            logger.info(f"üìù –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {answer}")

            if answer:
                logger.success("‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
                return True
            else:
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                return False

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False


def create_env_file():
    """–°–æ–∑–¥–∞–µ—Ç .env —Ñ–∞–π–ª —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è Ollama."""
    env_content = f"""# Live-Learn RAG Configuration

# App settings
APP_NAME="Live-Learn RAG with Llama"
APP_ENV=dev
DEBUG=true

# Database
DB_PATH=./rag.db

# LLM Configuration - Using Ollama with Llama
USE_OLLAMA=true
OLLAMA_URL={OLLAMA_URL}
OLLAMA_MODEL={OLLAMA_MODEL}

# Google AI Settings (disabled when using Ollama)
# GOOGLE_API_KEY=your_google_api_key_here
# LLM_MODEL=gemini-2.0-flash-exp
# EMBEDDING_MODEL=text-embedding-004

# Alternative: Use OpenAI for embeddings (optional)
USE_OPENAI_EMBEDDINGS=false
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# RAG settings
CHUNK_SIZE=400
CHUNK_OVERLAP=40
DEFAULT_TOP_K=6

# Feedback settings
FEEDBACK_PENALTY_WEIGHT=-0.3
FEEDBACK_BOOST_WEIGHT=0.5

# Security (optional)
# API_KEY=your_api_key_here
# API_KEY_HEADER=X-API-Key

# Performance
REQUEST_TIMEOUT=60
MAX_RETRIES=3

# LLM parameters
GEMINI_TEMPERATURE=0.1
GEMINI_MAX_TOKENS=8192
GEMINI_TOP_P=0.8
GEMINI_TOP_K=40
"""

    try:
        with open(ENV_FILE, "w", encoding="utf-8") as f:
            f.write(env_content)
        logger.success(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª {ENV_FILE} —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ Ollama")
        return True
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å {ENV_FILE}: {e}")
        return False


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama."""
    global OLLAMA_MODEL  # <-- –í–ê–ñ–ù–û: –æ–±—ä—è–≤–ª—è–µ–º –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    logger.info("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama –¥–ª—è Live-Learn RAG —Å–∏—Å—Ç–µ–º—ã")
    logger.info("=" * 60)

    # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É Ollama
    logger.info("üìã –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama")
    if not check_ollama_installed():
        install_ollama_instructions()
        return 1

    # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä
    logger.info("=" * 60)
    logger.info("üìã –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ Ollama")
    if not await check_ollama_running():
        logger.info("–ü—ã—Ç–∞–µ–º—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä...")
        if not start_ollama_server():
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä")
            logger.info("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é: ollama serve")
            return 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑ –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞
        await asyncio.sleep(2)
        if not await check_ollama_running():
            logger.error("‚ùå –°–µ—Ä–≤–µ—Ä –≤—Å–µ –µ—â–µ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
            return 1

    # –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
    logger.info("=" * 60)
    logger.info("üìã –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    models_info = await list_available_models()

    # –®–∞–≥ 4: –°–∫–∞—á–∏–≤–∞–µ–º –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    if OLLAMA_MODEL not in models_info["models"]:
        logger.info("=" * 60)
        logger.info(f"üìã –®–∞–≥ 4: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {OLLAMA_MODEL}")

        logger.info("ü§î –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è:")
        logger.info(f"   1. {OLLAMA_MODEL} (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è, ~2GB)")
        logger.info("   2. llama3.2:3b (–±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è, ~2GB)")
        logger.info("   3. llama3.2:1b (—Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è, ~1GB)")

        choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1): ").strip()

        if choice == "2":
            model_to_pull = "llama3.2:3b"
        elif choice == "3":
            model_to_pull = "llama3.2:1b"
        else:
            model_to_pull = OLLAMA_MODEL

        if not await pull_model(model_to_pull):
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å")
            return 1

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (—É–∂–µ –º–æ–∂–Ω–æ ‚Äî global –æ–±—ä—è–≤–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–µ)
        OLLAMA_MODEL = model_to_pull
    else:
        logger.success(f"‚úÖ –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞")

    # –®–∞–≥ 5: –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    logger.info("=" * 60)
    logger.info("üìã –®–∞–≥ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    if not await test_model(OLLAMA_MODEL):
        logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ —Ç–µ—Å—Ç")
        return 1

    # –®–∞–≥ 6: –°–æ–∑–¥–∞–µ–º .env —Ñ–∞–π–ª
    logger.info("=" * 60)
    logger.info("üìã –®–∞–≥ 6: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    if not create_env_file():
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª")
        return 1

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    logger.info("=" * 60)
    logger.success("üéâ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    logger.info("")
    logger.info("üìã –ß—Ç–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ:")
    logger.info(f"   ‚Ä¢ Ollama —Å–µ—Ä–≤–µ—Ä: {OLLAMA_URL}")
    logger.info(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: {OLLAMA_MODEL}")
    logger.info(f"   ‚Ä¢ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {ENV_FILE}")
    logger.info("")
    logger.info("üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    logger.info("   1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ RAG —Å–µ—Ä–≤–µ—Ä: python -m app.main")
    logger.info("   2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ: python load_full_data.py")
    logger.info("   3. –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –≤ –±—Ä–∞—É–∑–µ—Ä–µ")
    logger.info("")
    logger.info("üí° –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
    logger.info("   ‚Ä¢ –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: ollama list")
    logger.info("   ‚Ä¢ –ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é: ollama run " + OLLAMA_MODEL)
    logger.info("   ‚Ä¢ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: Ctrl+C –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –≥–¥–µ –∑–∞–ø—É—â–µ–Ω ollama serve")

    return 0


if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger.remove()
    logger.add(
        lambda msg: print(msg, end=''),
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level="INFO",
        colorize=True
    )

    # –ó–∞–ø—É—Å–∫
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("\nüõë –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
